import pandas as pd
from PIL import Image
import torch
import torch.nn.functional as F
from torchvision import transforms
from models.model import HerringModel
from models.multitask_model import MultiTaskHerringModel
from utils.population_mapper import PopulationMapper  # ðŸŸ¢ DODANO
from omegaconf import OmegaConf # <-- Dodano import
from pathlib import Path # <-- Dodano import

def run_full_dataset_prediction(loss_name: str, model_path: str, path_manager, log_dir, full_name):
    import pandas as pd
    from PIL import Image
    import torch
    import torch.nn.functional as F
    from torchvision import transforms
    from models.model import HerringModel
    from models.multitask_model import MultiTaskHerringModel
    from utils.population_mapper import PopulationMapper

    # Konwersja log_dir do Path, jeÅ›li jest stringiem
    log_dir_path = Path(log_dir)
    params_file = log_dir_path / "params.yaml"

    if params_file.exists():
        print(f"Åadowanie konfiguracji z pliku: {params_file}")
        cfg = OmegaConf.load(params_file)
    else:
        print(f"OSTRZEÅ»ENIE: Plik konfiguracyjny {params_file} nie znaleziony. UÅ¼ywam konfiguracji z path_manager.")
        if path_manager is None or path_manager.cfg is None:
            raise ValueError("Nie moÅ¼na zaÅ‚adowaÄ‡ konfiguracji: params.yaml nie istnieje, a path_manager lub path_manager.cfg jest None.")
        cfg = path_manager.cfg

    # PathManager moÅ¼e nadal byÄ‡ potrzebny do uzyskania project_root, jeÅ›li Å›cieÅ¼ki w cfg sÄ… wzglÄ™dne
    # lub do innych zadaÅ„. JeÅ›li path_manager jest None, a potrzebny, to bÅ‚Ä…d.
    if path_manager is None and not params_file.exists():
        # JeÅ›li nie ma params.yaml i nie ma path_manager, to nie mamy ani cfg, ani sposobu na Å›cieÅ¼ki.
        raise ValueError("Nie moÅ¼na zaÅ‚adowaÄ‡ konfiguracji ani uzyskaÄ‡ Å›cieÅ¼ek: params.yaml nie istnieje, a path_manager jest None.")

    if path_manager is None and params_file.exists():
        # Mamy cfg z params.yaml, ale nie mamy project_root do utworzenia nowego PathManagera.
        # W tej sytuacji zakÅ‚adamy, Å¼e Å›cieÅ¼ki w params.yaml sÄ… absolutne lub praca bez PathManagera jest niemoÅ¼liwa.
        # To jest maÅ‚o prawdopodobny scenariusz, jeÅ›li funkcja jest wywoÅ‚ywana z trainer_setup.
        # Dla bezpieczeÅ„stwa, rzuÄ‡my bÅ‚Ä…d, jeÅ›li path_manager jest potrzebny do okreÅ›lenia project_root.
        # MoÅ¼na by zmodyfikowaÄ‡ PathManager, aby nie wymagaÅ‚ project_root, jeÅ›li Å›cieÅ¼ki w cfg sÄ… absolutne.
        # Lub params.yaml musiaÅ‚by przechowywaÄ‡ project_root.
        # Na razie: jeÅ›li params.yaml istnieje, ale path_manager nie, to jest to problem dla PathManagera.
        # Chyba Å¼e cfg z params.yaml zawiera wszystkie potrzebne Å›cieÅ¼ki jako absolutne.
        print("OSTRZEÅ»ENIE: path_manager jest None. ÅšcieÅ¼ki w zaÅ‚adowanej konfiguracji cfg muszÄ… byÄ‡ absolutne lub poprawnie skonfigurowane.")
        # W tej sytuacji, jeÅ›li cfg.data.root_dir i cfg.data.metadata_file sÄ… np. wzglÄ™dne, to PathManager by ich nie rozwiÄ…zaÅ‚ poprawnie.
        # Dla uproszczenia, na razie zakÅ‚adamy, Å¼e jeÅ›li path_manager jest None, to cfg musi zawieraÄ‡ absolutne Å›cieÅ¼ki.
        # Lepszym rozwiÄ…zaniem byÅ‚oby przekazanie project_root jako argumentu.
        # Na razie, jeÅ›li path_manager jest None, nie tworzymy nowego. To spowoduje bÅ‚Ä…d pÃ³Åºniej, jeÅ›li metody path_manager bÄ™dÄ… wywoÅ‚ane.
        # To jest zÅ‚e. PathManager jest potrzebny.
        raise ValueError("path_manager jest None. Jest on wymagany do dziaÅ‚ania tej funkcji.")


    # UtwÃ³rz nowÄ… instancjÄ™ PathManager z zaÅ‚adowanÄ… konfiguracjÄ… cfg
    # Potrzebujemy project_root z oryginalnego path_manager
    # To jest kluczowe: oryginalny path_manager dostarcza project_root.
    from utils.path_manager import PathManager as PM_local # UnikniÄ™cie konfliktu nazw, jeÅ›li PathManager byÅ‚by teÅ¼ tu importowany
    current_pm = PM_local(project_root=path_manager.project_root, cfg=cfg)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    population_mapper = PopulationMapper(cfg.data.active_populations)

    # Ustal typ modelu
    is_multitask = getattr(cfg, "multitask_model", {}).get("use", False)
    if is_multitask:
        model = MultiTaskHerringModel(cfg).to(device)
    else:
        model = HerringModel(cfg).to(device)

    checkpoint = torch.load(model_path, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    model.eval()

    # UÅ¼yj cfg (zaÅ‚adowanego z params.yaml lub fallback) do okreÅ›lenia image_size
    image_size_to_use = cfg.multitask_model.backbone_model.image_size if is_multitask else cfg.base_model.image_size
    transform = transforms.Compose([
        transforms.Resize(image_size_to_use), # UÅ¼yj poprawnego image_size
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    excel_path = current_pm.metadata_file() # UÅ¼yj current_pm
    df = pd.read_excel(excel_path)
    if "FilePath" not in df.columns: # Pozostaje bez zmian
        raise ValueError("Brakuje kolumny 'FilePath' zawierajÄ…cej Å›cieÅ¼ki do obrazÃ³w")

    data_root = current_pm.data_root() # UÅ¼yj current_pm
    folders = [f"train/{pop}" for pop in population_mapper.active_populations] + \
              [f"val/{pop}" for pop in population_mapper.active_populations] + \
              [f"test/{pop}" for pop in population_mapper.active_populations]
    all_image_paths = []
    for folder in folders:
        folder_path = data_root / folder
        if folder_path.exists():
            all_image_paths.extend(folder_path.glob("*.jpg"))

    predictions = {}
    total = len(all_image_paths)
    print(f"\nðŸ” Start predykcji ({loss_name}) na {total} obrazach...")

    for i, image_path in enumerate(all_image_paths, 1):
        try:
            if i % 100 == 0 or i == total:
                print(f"Przetworzono {i} z {total} obrazÃ³w...")
            image = Image.open(image_path).convert("RGB")
            input_tensor = transform(image).unsqueeze(0).to(device)
            with torch.no_grad():
                if is_multitask:
                    pop_logits, age_pred = model(input_tensor)
                    probs = F.softmax(pop_logits, dim=1)[0]
                    pred_idx = pop_logits.argmax().item()
                    pred_class = population_mapper.to_pop(pred_idx)
                    confidence = float(probs[pred_idx]) * 100
                    predicted_age = float(age_pred[0].item())  # <-- WIEK jako liczba zmiennoprzecinkowa
                else:
                    pop_logits = model(input_tensor)
                    probs = F.softmax(pop_logits, dim=1)[0]
                    pred_idx = pop_logits.argmax().item()
                    pred_class = population_mapper.to_pop(pred_idx)
                    confidence = float(probs[pred_idx]) * 100
                    predicted_age = None  # brak predykcji wieku
            key = image_path.name.lower()
            predictions[key] = (pred_class, round(confidence, 2), predicted_age)
        except Exception as e:
            print(f"BÅ‚Ä…d przetwarzania obrazu {image_path}: {e}")

    pred_column = f"{loss_name}_pred"
    prob_column = f"{loss_name}_prob"
    age_column = f"{loss_name}_age_pred"
    pred_classes, pred_probs, pred_ages, not_found = [], [], [], []

    for file_name in df["FileName"]:
        key = str(file_name).lower()
        pred = predictions.get(key, (None, None, None))
        if pred[0] is None:
            not_found.append(key)
        pred_classes.append(pred[0])
        pred_probs.append(pred[1])
        pred_ages.append(pred[2])

    df[pred_column] = pred_classes
    df[prob_column] = pred_probs
    if is_multitask:
        df[age_column] = pred_ages

    output_path = log_dir / f"{full_name}_predictions.xlsx"
    if output_path.exists():
        existing_df = pd.read_excel(output_path)
        for col in [pred_column, prob_column, age_column]:
            if col in existing_df.columns:
                existing_df = existing_df.drop(columns=[col], errors='ignore')
        # Dodajemy tylko nowe kolumny
        if is_multitask:
            df = pd.concat([existing_df, df[[pred_column, prob_column, age_column]]], axis=1)
        else:
            df = pd.concat([existing_df, df[[pred_column, prob_column]]], axis=1)

    df.to_excel(output_path, index=False)
    print(f"\nðŸ“Š Liczba przetworzonych obrazÃ³w: {len(predictions)}")
    print(f"âœ… Zapisano predykcje ({loss_name}) do: {output_path}")
    print("â­ï¸ PrzechodzÄ™ do kolejnej funkcji straty...\n")

